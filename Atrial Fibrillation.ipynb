{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0bb7f94",
   "metadata": {},
   "source": [
    "# Atrial Fibrillation\n",
    "This notebook contains all of the code needed to set up and run SMoLK models on the atrial fibrillation detection task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d6f91c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Here, we import all of the necessary libraries we need to run this code, as well as set the random seed for reproducibility. Note that we set device to \"cuda\" to run on a CUDA-enabled GPU. If you do not have a CUDA enabled GPU, you can set this to \"cpu\" instead, though it will be much slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f62bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # path/directory stuff\n",
    "import pickle\n",
    "\n",
    "# Deep learning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Math\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.signal import periodogram\n",
    "from utils.stats import calculate_metrics, print_table\n",
    "\n",
    "# Data\n",
    "from utils.datasets import ZhengEtAl, CinC, generate_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"mps\" # device to use"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2ba0f56",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, class_names = CinC(base_dir=\"../Interpretable Arrhythmia.nosync/\")\n",
    "\n",
    "# exclude class 3, which is the noisy class\n",
    "X = X[Y != 3]\n",
    "Y = Y[Y != 3]\n",
    "class_names = [\"Normal\", \"Afib\", \"Other\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21b428bc",
   "metadata": {},
   "source": [
    "# Define the model, train, and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1800aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedFilters(nn.Module):\n",
    "    def __init__(self, num_kernels=24, num_classes=4):\n",
    "        super(LearnedFilters, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, num_kernels, 192, stride=1, bias=True)\n",
    "        self.conv2 = nn.Conv1d(1, num_kernels, 96, stride=1, bias=True)\n",
    "        self.conv3 = nn.Conv1d(1, num_kernels, 64, stride=1, bias=True)\n",
    "        \n",
    "        self.linear = nn.Linear(num_kernels*3 + 321, num_classes) # 321 is the size of the power spectrum\n",
    "    \n",
    "    def forward(self, x, powerspectrum):\n",
    "        c1 = F.leaky_relu(self.conv1(x)).mean(dim=-1)\n",
    "        c2 = F.leaky_relu(self.conv2(x)).mean(dim=-1)\n",
    "        c3 = F.leaky_relu(self.conv3(x)).mean(dim=-1)\n",
    "        \n",
    "        aggregate = torch.cat([c1,c2,c3, powerspectrum], dim=1)\n",
    "        aggregate = self.linear(aggregate)\n",
    "        \n",
    "        return aggregate\n",
    "\n",
    "def train(device, X, Y, class_weights=None, num_kernels=128, lr=0.001, batch_size=256, num_epoch=16, end_factor=0.1, use_tqdm=True):\n",
    "    # compute power spectra for X\n",
    "    PowerSpectra = []\n",
    "    for i in tqdm(range(0, len(X))):\n",
    "        PowerSpectra.append(periodogram(X[i], fs=64)[1])\n",
    "    PowerSpectra = np.float32(PowerSpectra)\n",
    "\n",
    "    model = LearnedFilters(num_kernels=num_kernels, num_classes=np.max(Y)+1).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=end_factor, total_iters=num_epoch*len(X)//batch_size)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float, device=device))\n",
    "    \n",
    "    if use_tqdm:\n",
    "        pbar = tqdm(range(0, num_epoch))\n",
    "    else:\n",
    "        pbar = range(0, num_epoch)\n",
    "    for epoch in pbar:\n",
    "        for batch_idx in range(0, len(X), batch_size):\n",
    "            data = X[batch_idx:batch_idx+batch_size]\n",
    "            powerspectrum = PowerSpectra[batch_idx:batch_idx+batch_size]\n",
    "            target = Y[batch_idx:batch_idx+batch_size]\n",
    "\n",
    "            data, powerspectrum, target = torch.tensor(data).to(device), torch.tensor(powerspectrum, dtype=torch.float32).to(device), torch.tensor(target).to(device).long()\n",
    "\n",
    "            data = data.unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data, powerspectrum)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if use_tqdm:\n",
    "                pbar.set_description(f\"loss: {loss.item():.5f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def test(model, device, X):\n",
    "    # compute power spectra for X\n",
    "    PowerSpectra = []\n",
    "    for i in range(0, len(X)):\n",
    "        PowerSpectra.append(periodogram(X[i], fs=64)[1])\n",
    "    PowerSpectra = np.float32(PowerSpectra)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = []\n",
    "        for i in range(0, len(X)):\n",
    "            data = X[i]\n",
    "            powerspectrum = PowerSpectra[i]\n",
    "            \n",
    "            data, powerspectrum = torch.tensor(data).to(device), torch.tensor(powerspectrum, dtype=torch.float32).to(device)\n",
    "            data = data.unsqueeze(0).unsqueeze(1)\n",
    "            powerspectrum = powerspectrum.unsqueeze(0)\n",
    "            \n",
    "            output = model(data, powerspectrum).softmax(dim=-1)\n",
    "            probs.append(output.cpu().numpy())\n",
    "\n",
    "    probs = np.concatenate(probs, axis=0)\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a38a7c2",
   "metadata": {},
   "source": [
    "# Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "num_splits = 10\n",
    "data_fraction = 1.0\n",
    "num_kernels = 128\n",
    "for split in range(0, num_splits):\n",
    "    print(f\"Split {split + 1}\")\n",
    "    X_train, Y_train, X_test, Y_test = generate_split(X, Y, split, num_splits)\n",
    "\n",
    "    # shuffle X_train and Y_train\n",
    "    p = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[p]\n",
    "    Y_train = Y_train[p]\n",
    "\n",
    "    X_train = X_train[:int(data_fraction * len(X_train))]\n",
    "    Y_train = Y_train[:int(data_fraction * len(Y_train))]\n",
    "\n",
    "    class_weights = 1 / np.bincount(Y_train)\n",
    "    class_weights /= class_weights.sum()\n",
    "\n",
    "    model = train(device, X_train, Y_train, class_weights=class_weights, num_kernels=num_kernels, lr=0.1, batch_size=1024, num_epoch=512, end_factor=0.0, use_tqdm=True)\n",
    "    models.append(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43a0d317",
   "metadata": {},
   "source": [
    "# Test the models via Cross Validation\n",
    "In this section, we simply run each model fold again the various test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b91424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models\n",
    "probs = []\n",
    "ground_truth = []\n",
    "for split in range(0, num_splits):\n",
    "    print(f\"Split {split + 1}\")\n",
    "    _, _, X_test, Y_test = generate_split(X, Y, split, num_splits)\n",
    "    with torch.no_grad():\n",
    "        Y_prob = test(models[split], device, X_test)\n",
    "        Y_prob = torch.tensor(Y_prob).cpu().numpy()\n",
    "\n",
    "    probs.append(Y_prob)\n",
    "    ground_truth.append(Y_test)\n",
    "\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "AUCs = []\n",
    "F1s = []\n",
    "\n",
    "for split in range(0, num_splits):\n",
    "    sen, spec, auc, f1 = calculate_metrics(ground_truth[split], probs[split], num_classes=Y.max()+1)\n",
    "    sensitivities.append(sen)\n",
    "    specificities.append(spec)\n",
    "    AUCs.append(auc)\n",
    "    F1s.append(f1)\n",
    "\n",
    "sensitivities = np.array(sensitivities)\n",
    "specificities = np.array(specificities)\n",
    "AUCs = np.array(AUCs)\n",
    "F1s = np.array(F1s)\n",
    "\n",
    "print_table(sensitivities, specificities, AUCs, class_names)\n",
    "print(f\"F1: {F1s.mean():.3f} ± {F1s.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871d0e63",
   "metadata": {},
   "source": [
    "# Test on Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c5cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, class_names = ZhengEtAl(base_dir=\"../Interpretable Arrhythmia.nosync/\")\n",
    "\n",
    "class_names = [\"Normal\", \"Afib\", \"Other\"]\n",
    "new_mapping = [2, 2, 0, 1, 2, 2, 2] # our loader outputs many different classes, so we want to condense the other classes to \"other\"\n",
    "Y = np.array([new_mapping[y] for y in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb9d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models\n",
    "probs = []\n",
    "ground_truth = []\n",
    "for split in range(0, num_splits):\n",
    "    print(f\"Split {split + 1}\")\n",
    "    with torch.no_grad():\n",
    "        Y_prob = test(models[split], device, X)\n",
    "        Y_prob = torch.tensor(Y_prob).cpu().numpy()\n",
    "\n",
    "    probs.append(Y_prob)\n",
    "    ground_truth.append(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivities = []\n",
    "specificities = []\n",
    "AUCs = []\n",
    "F1s = []\n",
    "\n",
    "for split in range(0, num_splits):\n",
    "    sen, spec, auc, f1 = calculate_metrics(ground_truth[split], probs[split], num_classes=Y.max()+1)\n",
    "    sensitivities.append(sen)\n",
    "    specificities.append(spec)\n",
    "    AUCs.append(auc)\n",
    "    F1s.append(f1)\n",
    "\n",
    "sensitivities = np.array(sensitivities)\n",
    "specificities = np.array(specificities)\n",
    "AUCs = np.array(AUCs)\n",
    "F1s = np.array(F1s)\n",
    "\n",
    "print_table(sensitivities, specificities, AUCs, class_names)\n",
    "print(f\"F1: {F1s.mean():.3f} ± {F1s.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc28e28",
   "metadata": {},
   "source": [
    "# ResNet Model\n",
    "Now, let's compare against a ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.misc import train, test # NOTE: THIS OVERWRITES THE PREVIOUS train AND test FUNCTIONS\n",
    "\n",
    "# code is ported from https://github.com/antonior92/automatic-ecg-diagnosis/tree/master?tab=readme-ov-file\n",
    "\n",
    "class ResidualUnit(nn.Module):\n",
    "    def __init__(self, n_filters_in, n_filters_out, kernel_size=17, stride=1, dropout_rate=0.2, \n",
    "                 preactivation=True, postactivation_bn=False, activation_function='relu'):\n",
    "        super(ResidualUnit, self).__init__()\n",
    "        self.preactivation = preactivation\n",
    "        self.postactivation_bn = postactivation_bn\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Activation function\n",
    "        if activation_function == 'relu':\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Activation function '{}' not implemented.\".format(activation_function))\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(n_filters_in)\n",
    "        self.conv1 = nn.Conv1d(n_filters_in, n_filters_out, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(n_filters_out)\n",
    "        self.conv2 = nn.Conv1d(n_filters_out, n_filters_out, kernel_size, stride=stride, padding=kernel_size//2-1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.downsample = stride != 1 or n_filters_in != n_filters_out\n",
    "        if self.downsample:\n",
    "            self.conv_shortcut = nn.Conv1d(n_filters_in, n_filters_out, 1, stride=stride, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = x\n",
    "        if self.preactivation:\n",
    "            out = self.bn1(out)\n",
    "            out = self.activation(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.activation(out)\n",
    "        if self.dropout_rate > 0:\n",
    "            out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        if self.downsample:\n",
    "            identity = self.conv_shortcut(identity)\n",
    "\n",
    "        out += identity\n",
    "        if not self.preactivation or self.postactivation_bn:\n",
    "            out = self.bn2(out)\n",
    "            out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, n_classes, kernel_size=16, last_layer_activation='sigmoid'):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.res1 = ResidualUnit(64, 128, kernel_size=kernel_size, stride=3)\n",
    "        self.res2 = ResidualUnit(128, 196, kernel_size=kernel_size, stride=3)\n",
    "        self.res3 = ResidualUnit(196, 256, kernel_size=kernel_size, stride=2)\n",
    "        self.res4 = ResidualUnit(256, 320, kernel_size=kernel_size, stride=2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(5760, n_classes)\n",
    "        \n",
    "        # Last layer activation\n",
    "        if last_layer_activation == 'sigmoid':\n",
    "            self.last_activation = nn.Sigmoid()\n",
    "        elif last_layer_activation == 'softmax':\n",
    "            # Softmax is typically used with nn.CrossEntropyLoss, which expects raw scores.\n",
    "            self.last_activation = lambda x: F.softmax(x, dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Last layer activation '{}' not implemented.\".format(last_layer_activation))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.res4(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        #x = self.last_activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1928d3c9",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200cf30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, class_names = CinC(base_dir=\"../Interpretable Arrhythmia.nosync/\")\n",
    "# exclude class 3\n",
    "X = X[Y != 3]\n",
    "Y = Y[Y != 3]\n",
    "class_names = [\"Normal\", \"Afib\", \"Other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dffd76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "num_splits = 10\n",
    "data_fraction = 1.0\n",
    "for split in range(0, num_splits):\n",
    "    print(f\"Split {split + 1}\")\n",
    "    X_train, Y_train, X_test, Y_test = generate_split(X, Y, split, num_splits)\n",
    "\n",
    "    # shuffle X_train and Y_train\n",
    "    p = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[p]\n",
    "    Y_train = Y_train[p]\n",
    "\n",
    "    X_train = X_train[:int(data_fraction * len(X_train))]\n",
    "    Y_train = Y_train[:int(data_fraction * len(Y_train))]\n",
    "\n",
    "    class_weights = 1 / np.bincount(Y_train)\n",
    "    class_weights /= class_weights.sum()\n",
    "\n",
    "    model = ResNet1D(n_classes=len(class_names)).to(device)\n",
    "    model = train(model, device, X_train, Y_train, class_weights=class_weights, lr=0.001, batch_size=1024, num_epoch=512, end_factor=0.0, use_tqdm=True)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdff4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models\n",
    "probs = []\n",
    "ground_truth = []\n",
    "for split in range(0, num_splits):\n",
    "    print(f\"Split {split + 1}\")\n",
    "    _, _, X_test, Y_test = generate_split(X, Y, split, num_splits)\n",
    "    with torch.no_grad():\n",
    "        Y_prob = test(models[split], device, X_test)\n",
    "        Y_prob = torch.tensor(Y_prob).softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    probs.append(Y_prob)\n",
    "    ground_truth.append(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab800b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivities = []\n",
    "specificities = []\n",
    "AUCs = []\n",
    "F1s = []\n",
    "\n",
    "for split in range(0, num_splits):\n",
    "    sen, spec, auc, f1 = calculate_metrics(ground_truth[split], probs[split], num_classes=Y.max()+1)\n",
    "    sensitivities.append(sen)\n",
    "    specificities.append(spec)\n",
    "    AUCs.append(auc)\n",
    "    F1s.append(f1)\n",
    "\n",
    "sensitivities = np.array(sensitivities)\n",
    "specificities = np.array(specificities)\n",
    "AUCs = np.array(AUCs)\n",
    "F1s = np.array(F1s)\n",
    "\n",
    "print_table(sensitivities, specificities, AUCs, class_names)\n",
    "print(f\"F1: {F1s.mean():.3f} ± {F1s.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b0b1c",
   "metadata": {},
   "source": [
    "# Test on Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, class_names = ZhengEtAl(base_dir=\"../Interpretable Arrhythmia.nosync/\")\n",
    "\n",
    "class_names = [\"Normal\", \"Afib\", \"Other\"]\n",
    "new_mapping = [2, 2, 0, 1, 2, 2, 2]\n",
    "Y = np.array([new_mapping[y] for y in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6821a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models\n",
    "probs = []\n",
    "ground_truth = []\n",
    "for split in range(0, num_splits):\n",
    "    print(f\"Split {split + 1}\")\n",
    "    with torch.no_grad():\n",
    "        Y_prob = test(models[split], device, X)\n",
    "        Y_prob = torch.tensor(Y_prob).cpu().numpy()\n",
    "\n",
    "    probs.append(Y_prob)\n",
    "    ground_truth.append(Y)\n",
    "\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "AUCs = []\n",
    "F1s = []\n",
    "\n",
    "for split in range(0, num_splits):\n",
    "    sen, spec, auc, f1 = calculate_metrics(ground_truth[split], probs[split], num_classes=Y.max()+1)\n",
    "    sensitivities.append(sen)\n",
    "    specificities.append(spec)\n",
    "    AUCs.append(auc)\n",
    "    F1s.append(f1)\n",
    "\n",
    "sensitivities = np.array(sensitivities)\n",
    "specificities = np.array(specificities)\n",
    "AUCs = np.array(AUCs)\n",
    "F1s = np.array(F1s)\n",
    "\n",
    "print_table(sensitivities, specificities, AUCs, class_names)\n",
    "print(f\"F1: {F1s.mean():.3f} ± {F1s.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2aa1b",
   "metadata": {},
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ac91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import ZhengEtAl_AVB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286824dc",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154277e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, class_names = ZhengEtAl_AVB(base_dir=\"../Interpretable Arrhythmia.nosync/\")\n",
    "X = X[Y != 2]\n",
    "Y = Y[Y != 2]\n",
    "class_names = class_names[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec80966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle X_train and Y_train\n",
    "p = np.random.permutation(len(X))\n",
    "split = 0.8\n",
    "X = X[p]\n",
    "Y = Y[p]\n",
    "X_train = X[:int(len(X) * split)]\n",
    "Y_train = Y[:int(len(X) * split)]\n",
    "X_test = X[int(len(X) * split):]\n",
    "Y_test = Y[int(len(X) * split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5df584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = 1 / np.bincount(Y_train)\n",
    "class_weights /= class_weights.sum()\n",
    "\n",
    "def train(device, X, Y, class_weights=None, num_kernels=128, lr=0.001, batch_size=256, num_epoch=16, end_factor=0.1, use_tqdm=True):\n",
    "    # compute power spectra for X\n",
    "    PowerSpectra = []\n",
    "    for i in tqdm(range(0, len(X))):\n",
    "        PowerSpectra.append(periodogram(X[i], fs=64)[1])\n",
    "    PowerSpectra = np.float32(PowerSpectra)\n",
    "\n",
    "    model = LearnedFilters(num_kernels=num_kernels, num_classes=np.max(Y)+1).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=end_factor, total_iters=num_epoch*len(X)//batch_size)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float, device=device))\n",
    "    \n",
    "    if use_tqdm:\n",
    "        pbar = tqdm(range(0, num_epoch))\n",
    "    else:\n",
    "        pbar = range(0, num_epoch)\n",
    "    for epoch in pbar:\n",
    "        for batch_idx in range(0, len(X), batch_size):\n",
    "            data = X[batch_idx:batch_idx+batch_size]\n",
    "            powerspectrum = PowerSpectra[batch_idx:batch_idx+batch_size]\n",
    "            target = Y[batch_idx:batch_idx+batch_size]\n",
    "\n",
    "            data, powerspectrum, target = torch.tensor(data).to(device), torch.tensor(powerspectrum, dtype=torch.float32).to(device), torch.tensor(target).to(device).long()\n",
    "\n",
    "            data = data.unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data, powerspectrum)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if use_tqdm:\n",
    "                pbar.set_description(f\"loss: {loss.item():.5f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "lkm_model = train(device, X_train, Y_train, class_weights=class_weights, num_kernels=128, lr=0.1, batch_size=1024, num_epoch=512, end_factor=0.0, use_tqdm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbe24b",
   "metadata": {},
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facbcd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute power spectra for X_train\n",
    "from scipy.signal import periodogram\n",
    "from tqdm import tqdm\n",
    "\n",
    "PowerSpectra = []\n",
    "for i in tqdm(range(0, len(X))):\n",
    "    PowerSpectra.append(periodogram(X[i], fs=64)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c58997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "COI = 1\n",
    "\n",
    "indices = np.arange(len(X))[Y==COI]\n",
    "idx = np.random.choice(indices)\n",
    "print(idx)\n",
    "\n",
    "num_kernels = 128\n",
    "net = lkm_model\n",
    "sd = lkm_model.state_dict()\n",
    "\n",
    "#size = 5\n",
    "#order = 3\n",
    "x = torch.tensor(X[idx], dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "c1 = F.leaky_relu(net.conv1(x)[0])\n",
    "c2 = F.leaky_relu(net.conv2(x)[0])\n",
    "c3 = F.leaky_relu(net.conv3(x)[0])\n",
    "\n",
    "x = x[0, 0]\n",
    "feature_contribution = (torch.cat([c1.mean(dim=-1),c2.mean(dim=-1),c3.mean(dim=-1)]) * sd['linear.weight'][COI, :num_kernels*3]).sum().item()\n",
    "spectral_contribution = np.sum(PowerSpectra[idx] * sd['linear.weight'][COI, -321:].detach().cpu().numpy())\n",
    "\n",
    "with torch.no_grad():\n",
    "    accum = torch.zeros_like(x)\n",
    "    for i in range(num_kernels):\n",
    "        for j in range(x.shape[-1] - 192 + 1):\n",
    "            accum[j:j+192] += c1[i, j] * sd['linear.weight'][COI, i] / 192\n",
    "        for j in range(x.shape[-1] - 96 + 1):\n",
    "            accum[j:j+96] += c2[i, j] * sd['linear.weight'][COI, i+num_kernels] / 96\n",
    "        for j in range(x.shape[-1] - 64 + 1):\n",
    "            accum[j:j+64] += c3[i, j] * sd['linear.weight'][COI, i+num_kernels*2] / 64\n",
    "\n",
    "accum = accum.cpu().numpy()[192//2:-192//2]\n",
    "x = x.cpu().numpy()[192//2:-192//2]\n",
    "\n",
    "plt.figure(figsize=(5, 1), dpi=300)\n",
    "#plt.title(f\"True label: {class_names[Y[idx]]}, Class of interest: {class_names[COI]}, Contribution: {feature_contribution:.2f}\")\n",
    "#plt.title(\"LKM\")\n",
    "#norm = plt.Normalize(-np.abs(accum).max(), np.abs(accum).max())\n",
    "norm = plt.Normalize(accum.min(), accum.max())\n",
    "\n",
    "num_interpolated_points = len(x) * 100  # 100 times the number of original points\n",
    "linspace = np.linspace(0, len(x)-1, num_interpolated_points)\n",
    "accum_interp = np.interp(linspace, np.arange(len(x)), accum)\n",
    "point_sizes = np.interp(np.abs(accum_interp), (np.abs(accum_interp).min(), np.abs(accum_interp).max()), (10, 100))\n",
    "x_interp = np.interp(linspace, np.arange(0, len(x)), x)\n",
    "\n",
    "# Choose a suitable colormap (you can use any other colormap as needed)\n",
    "colormap = plt.get_cmap('RdBu_r')\n",
    "\n",
    "# Plot the curve\n",
    "plt.plot(np.arange(0, len(x)), x, c='k', linewidth=0.5)  # Using black curve for better visualization\n",
    "\n",
    "# Overlay with colored points\n",
    "point_sizes = np.interp(np.abs(accum_interp), (np.abs(accum_interp).min(), np.abs(accum_interp).max()), (0.01, 0.5))\n",
    "plt.scatter(linspace, x_interp, c=accum_interp, cmap=colormap, norm=norm, s=point_sizes)\n",
    "\n",
    "# Show colorbar\n",
    "cbar = plt.colorbar()\n",
    "#cbar.set_label('Contribution')\n",
    "\n",
    "# Remove axis ticks\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlim(0, len(x)-1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 2), dpi=150)\n",
    "plt.title(f\"Frequency Spectrum, Contribution: {spectral_contribution:.2f}\")\n",
    "\n",
    "weight = sd['linear.weight'][COI, -321:].cpu().numpy() * PowerSpectra[idx]\n",
    "\n",
    "# Normalize weight for color mapping\n",
    "norm = plt.Normalize(weight.min(), weight.max())\n",
    "colormap = plt.get_cmap('RdBu_r')\n",
    "colors = colormap(norm(weight))\n",
    "\n",
    "plt.bar(np.linspace(0, 32, len(PowerSpectra[idx])), PowerSpectra[idx], color=colors, width=1/len(PowerSpectra[idx])*32*2)\n",
    "plt.xlim(0, 32)\n",
    "plt.xlabel(\"Frequency (Hz)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2605d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
